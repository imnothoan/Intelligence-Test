# H∆∞·ªõng D·∫´n Chi Ti·∫øt v·ªÅ Training Models & Dataset üéì

## M·ª§C L·ª§C

1. [T·ªïng Quan - B·∫†N C√ì C·∫¶N TRAIN KH√îNG?](#1-t·ªïng-quan)
2. [CAT Algorithm - Calibration](#2-cat-algorithm)
3. [Anti-Cheat Model - Computer Vision](#3-anti-cheat-model)
4. [Essay Grading - LLM Integration](#4-essay-grading)
5. [Dataset - L·∫•y ·ªû ƒê√¢u?](#5-dataset)
6. [Google Colab Training](#6-google-colab)
7. [Fine-tuning LLMs](#7-fine-tuning-llms)

---

## 1. T·ªîNG QUAN

### ‚ùå B·∫†N KH√îNG C·∫¶N TRAIN!

**99% tr∆∞·ªùng h·ª£p, b·∫°n KH√îNG c·∫ßn train b·∫•t k·ª≥ model n√†o!**

H·ªá th·ªëng ƒë√£ t√≠ch h·ª£p s·∫µn:
- ‚úÖ **CAT Algorithm**: Ho·∫°t ƒë·ªông v·ªõi manual calibration
- ‚úÖ **Anti-Cheat**: D√πng BlazeFace (Google) - ƒë√£ train s·∫µn
- ‚úÖ **Essay Grading**: D√πng Gemini/OpenAI - kh√¥ng c·∫ßn train
- ‚úÖ **Question Generation**: D√πng Gemini/OpenAI - kh√¥ng c·∫ßn train

### ‚úÖ KHI N√ÄO C·∫¶N TRAIN?

**Ch·ªâ train khi:**
1. **CAT Calibration**: Sau khi c√≥ 100+ h·ªçc sinh l√†m b√†i ‚Üí C·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c
2. **Custom Anti-Cheat**: Ph√°t hi·ªán gian l·∫≠n ƒë·∫∑c th√π c·ªßa tr∆∞·ªùng b·∫°n
3. **Fine-tune LLM**: Domain-specific (m√¥n h·ªçc r·∫•t chuy√™n s√¢u)

### Quy Tr√¨nh L√†m Vi·ªác

```
B·∫ÆT ƒê·∫¶U
  ‚Üì
[1] C√†i ƒë·∫∑t h·ªá th·ªëng (npm install)
  ‚Üì
[2] Th√™m Gemini API key (MI·ªÑN PH√ç)
  ‚Üì
[3] S·ª≠ d·ª•ng ngay! ‚úÖ
  ‚Üì
(Optional) Sau 1-2 th√°ng
  ‚Üì
[4] Calibrate CAT v·ªõi d·ªØ li·ªáu th·ª±c
  ‚Üì
[5] Train custom anti-cheat (n·∫øu c·∫ßn)
```

---

## 2. CAT ALGORITHM

### 2.1. L√Ω Thuy·∫øt

CAT (Computerized Adaptive Testing) c·∫ßn **question difficulty** ƒë·ªÉ ho·∫°t ƒë·ªông.

**Difficulty Scale**: 0.0 (d·ªÖ nh·∫•t) ‚Üí 1.0 (kh√≥ nh·∫•t)

### 2.2. Manual Calibration (KHUY·∫æN NGH·ªä)

**C√°ch 1: G√°n Th·ªß C√¥ng**

Khi t·∫°o c√¢u h·ªèi, g√°n difficulty d·ª±a tr√™n ƒë√°nh gi√°:

```
EASY (0.0 - 0.3):
- Ki·∫øn th·ª©c c∆° b·∫£n
- Nh·ªõ v√† hi·ªÉu
- V√≠ d·ª•: "2 + 2 = ?"

MEDIUM (0.3 - 0.7):
- √Åp d·ª•ng ki·∫øn th·ª©c
- Ph√¢n t√≠ch
- V√≠ d·ª•: "Gi·∫£i ph∆∞∆°ng tr√¨nh b·∫≠c 2"

HARD (0.7 - 1.0):
- T·ªïng h·ª£p, ƒë√°nh gi√°
- T∆∞ duy ph·∫£n bi·ªán
- V√≠ d·ª•: "Ch·ª©ng minh ƒë·ªãnh l√Ω..."
```

### 2.3. Data-Based Calibration (N√¢ng Cao)

**Khi n√†o**: Sau khi c√≥ ‚â•100 h·ªçc sinh l√†m b√†i

**B∆∞·ªõc 1: Xu·∫•t D·ªØ Li·ªáu**

```javascript
// Trong Analytics Dashboard
1. Ch·ªçn exam
2. Click "Export Data"
3. Download file CSV
```

**B∆∞·ªõc 2: Ch·∫°y Script Calibration**

```bash
cd docs/examples/training-scripts

# Install dependencies
pip install numpy pandas scipy

# Run calibration
python calibrate_cat.py ../../data/exam_responses.csv
```

**Script `calibrate_cat.py`**:

```python
import pandas as pd
import numpy as np
from scipy.optimize import minimize

def calculate_difficulty(responses_df):
    """
    Calculate difficulty for each question
    Formula: difficulty = 1 - (correct_count / total_count)
    """
    questions = responses_df.groupby('question_id').agg({
        'correct': ['sum', 'count']
    })
    
    questions['difficulty'] = 1 - (
        questions['correct']['sum'] / questions['correct']['count']
    )
    
    return questions['difficulty'].to_dict()

# Load data
df = pd.read_csv('exam_responses.csv')

# Calculate
difficulties = calculate_difficulty(df)

# Save results
import json
with open('difficulties.json', 'w') as f:
    json.dump(difficulties, f, indent=2)

print("Calibration complete! See difficulties.json")
```

**B∆∞·ªõc 3: Import V√†o Question Bank**

```javascript
// Trong Question Bank UI
1. Click "Import Difficulties"
2. Upload difficulties.json
3. H·ªá th·ªëng t·ª± ƒë·ªông c·∫≠p nh·∫≠t
```

### 2.4. IRT-Based Calibration (Chuy√™n Gia)

**Y√™u c·∫ßu**: Python + R, 500+ responses

```python
# Install py-irt
pip install py-irt

# Script: train_cat_model.py
from py_irt import irt

# Load data
data = pd.read_csv('responses.csv')

# Train IRT model (1PL - Rasch Model)
model = irt(data, model='1pl')
difficulties = model.params['difficulty']

# Export
difficulties.to_json('irt_difficulties.json')
```

**V·ªõi R (n√¢ng cao h∆°n)**:

```r
# Install mirt package
install.packages("mirt")
library(mirt)

# Load data
data <- read.csv("responses.csv")

# Train model
model <- mirt(data, 1, itemtype = "Rasch")

# Extract parameters
params <- coef(model, simplify = TRUE)
difficulties <- params$items[, "d"]

# Save
write.json(difficulties, "difficulties.json")
```

---

## 3. ANTI-CHEAT MODEL

### 3.1. S·ª≠ D·ª•ng BlazeFace (KHUY·∫æN NGH·ªä)

**Kh√¥ng c·∫ßn training!** BlazeFace ƒë√£ ƒë∆∞·ª£c Google train v·ªõi 1M+ ·∫£nh.

**T√≠nh nƒÉng c√≥ s·∫µn**:
- ‚úÖ Ph√°t hi·ªán khu√¥n m·∫∑t
- ‚úÖ Theo d√µi chuy·ªÉn ƒë·ªông
- ‚úÖ Ph√°t hi·ªán nhi·ªÅu ng∆∞·ªùi
- ‚úÖ C·∫£nh b√°o khi kh√¥ng nh√¨n m√†n h√¨nh

### 3.2. Training Custom Model

**Khi n√†o c·∫ßn**: 
- Ph√°t hi·ªán h√†nh vi ƒë·∫∑c th√π (s·ª≠ d·ª•ng t√†i li·ªáu, ƒëi·ªán tho·∫°i, ...)
- M√¥i tr∆∞·ªùng ƒë·∫∑c bi·ªát (g√≥c m√°y, √°nh s√°ng kh√°c th∆∞·ªùng)

**B∆∞·ªõc 1: Thu Th·∫≠p D·ªØ Li·ªáu**

C·∫ßn 2 lo·∫°i ·∫£nh:

```
data/
‚îú‚îÄ‚îÄ normal_behavior/       # 500-1000 ·∫£nh
‚îÇ   ‚îú‚îÄ‚îÄ student_1.jpg      # Nh√¨n m√†n h√¨nh
‚îÇ   ‚îú‚îÄ‚îÄ student_2.jpg      # T·∫≠p trung l√†m b√†i
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ suspicious_behavior/   # 500-1000 ·∫£nh
    ‚îú‚îÄ‚îÄ cheat_1.jpg        # Nh√¨n xu·ªëng (t√†i li·ªáu)
    ‚îú‚îÄ‚îÄ cheat_2.jpg        # Nh√¨n sang (b·∫°n b√®)
    ‚îî‚îÄ‚îÄ ...
```

**C√°ch thu th·∫≠p**:
```python
# Script: collect_data.py
import cv2

cam = cv2.VideoCapture(0)
counter = 0

print("Press SPACE to capture, Q to quit")
while True:
    ret, frame = cam.read()
    cv2.imshow('Capture', frame)
    
    key = cv2.waitKey(1)
    if key == ord(' '):
        cv2.imwrite(f'normal_{counter}.jpg', frame)
        counter += 1
    elif key == ord('q'):
        break

cam.release()
cv2.destroyAllWindows()
```

**B∆∞·ªõc 2: Label D·ªØ Li·ªáu**

S·∫Øp x·∫øp ·∫£nh v√†o 2 folders: `normal/` v√† `suspicious/`

**B∆∞·ªõc 3: Train Model**

```python
# Script: train_anticheat_model.py
import tensorflow as tf
from tensorflow import keras
import os

# Load data
def load_images(normal_dir, suspicious_dir, img_size=128):
    images = []
    labels = []
    
    # Normal behavior = 0
    for img in os.listdir(normal_dir):
        img_path = os.path.join(normal_dir, img)
        img = keras.preprocessing.image.load_img(
            img_path, target_size=(img_size, img_size)
        )
        img_array = keras.preprocessing.image.img_to_array(img) / 255.0
        images.append(img_array)
        labels.append(0)
    
    # Suspicious behavior = 1
    for img in os.listdir(suspicious_dir):
        img_path = os.path.join(suspicious_dir, img)
        img = keras.preprocessing.image.load_img(
            img_path, target_size=(img_size, img_size)
        )
        img_array = keras.preprocessing.image.img_to_array(img) / 255.0
        images.append(img_array)
        labels.append(1)
    
    return np.array(images), np.array(labels)

# Load
X, y = load_images('data/normal_behavior', 'data/suspicious_behavior')

# Split train/test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Create model
model = keras.Sequential([
    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Conv2D(64, (3, 3), activation='relu'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Conv2D(64, (3, 3), activation='relu'),
    keras.layers.Flatten(),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(1, activation='sigmoid')
])

# Compile
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Train
history = model.fit(
    X_train, y_train,
    epochs=20,
    batch_size=32,
    validation_data=(X_test, y_test)
)

# Evaluate
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f'Test accuracy: {test_acc:.2f}')

# Save
model.save('anticheat_model.h5')
```

**B∆∞·ªõc 4: Convert to TensorFlow.js**

```bash
# Install converter
pip install tensorflowjs

# Convert
tensorflowjs_converter \
  --input_format keras \
  anticheat_model.h5 \
  ./public/models/anticheat
```

**B∆∞·ªõc 5: S·ª≠ D·ª•ng Trong App**

Model t·ª± ƒë·ªông ƒë∆∞·ª£c load t·ª´ `public/models/anticheat/`

---

## 4. ESSAY GRADING

### 4.1. S·ª≠ D·ª•ng Gemini (KHUY·∫æN NGH·ªä)

**Kh√¥ng c·∫ßn training!** Ch·ªâ c·∫ßn API key.

```javascript
// ƒê√£ t√≠ch h·ª£p s·∫µn!
import { geminiService } from '@/services/geminiService';

const result = await geminiService.gradeEssay(
  question,
  studentAnswer,
  rubric,
  maxScore
);
```

### 4.2. Prompt Engineering (T·ªëi ∆Øu)

**C·∫£i thi·ªán k·∫øt qu·∫£ KH√îNG C·∫¶N training**:

```javascript
// File: src/services/geminiService.ts
// T√¨m method buildEssayGradingPrompt

private buildEssayGradingPrompt(...) {
  return `B·∫°n l√† gi√°o vi√™n ${subject} c√≥ 20 nƒÉm kinh nghi·ªám.

C√¢u h·ªèi: ${question}

B√†i l√†m: ${studentAnswer}

Ti√™u ch√≠ ch·∫•m (${maxScore} ƒëi·ªÉm):
1. N·ªôi dung (40%): ${contentCriteria}
2. C·∫•u tr√∫c (30%): ${structureCriteria}
3. Ng√¥n ng·ªØ (20%): ${languageCriteria}
4. S√°ng t·∫°o (10%): ${creativityCriteria}

Y√äU C·∫¶U:
- Ch·∫•m ƒëi·ªÉm ch√≠nh x√°c theo ti√™u ch√≠
- Nh·∫≠n x√©t c·ª• th·ªÉ v·ªõi v√≠ d·ª•
- ƒê∆∞a ra 3 ƒëi·ªÉm m·∫°nh v√† 3 ƒëi·ªÉm c·∫ßn c·∫£i thi·ªán
- G·ª£i √Ω c√°ch h·ªçc t·ªët h∆°n

Tr·∫£ v·ªÅ JSON...`;
}
```

### 4.3. Fine-tuning (Chuy√™n Gia)

**Khi n√†o c·∫ßn**:
- M√¥n h·ªçc r·∫•t ƒë·∫∑c th√π
- Chu·∫©n ch·∫•m ƒëi·ªÉm ri√™ng
- C√≥ ‚â•1000 b√†i ƒë√£ ch·∫•m

**Dataset c·∫ßn**:
```json
[
  {
    "question": "Ph√¢n t√≠ch h√¨nh t∆∞·ª£ng...",
    "answer": "B√†i l√†m c·ªßa h·ªçc sinh...",
    "score": 85,
    "feedback": "B√†i l√†m t·ªët..."
  },
  // ... 1000+ samples
]
```

**Fine-tuning v·ªõi OpenAI**:

```bash
# Prepare data
python prepare_finetuning_data.py

# Upload to OpenAI
openai api fine_tunes.create \
  -t essay_grading_train.jsonl \
  -v essay_grading_val.jsonl \
  -m gpt-3.5-turbo \
  --suffix "essay-grader"

# Wait for completion (~1-2 hours)
# Cost: ~$20-50 for 1000 examples
```

**Fine-tuning v·ªõi Gemini** (hi·ªán ch∆∞a h·ªó tr·ª£):
- Google ch∆∞a m·ªü fine-tuning cho Gemini
- D√πng prompt engineering thay th·∫ø

---

## 5. DATASET

### 5.1. Ngu·ªìn Dataset Mi·ªÖn Ph√≠

#### Ti·∫øng Vi·ªát

**VLSP (Vietnamese Language and Speech Processing)**
- URL: http://vlsp.org.vn/resources
- N·ªôi dung: Text, QA, NER
- Format: JSON, CSV
- Download: ƒêƒÉng k√Ω mi·ªÖn ph√≠

**UIT-ViQuAD (Vietnamese Question Answering)**
- URL: https://github.com/uitnlp/ViQuAD
- N·ªôi dung: 23K c√¢u h·ªèi ti·∫øng Vi·ªát
- Format: JSON
- License: MIT

**Vietnamese Wikipedia**
- URL: https://dumps.wikimedia.org/viwiki/
- N·ªôi dung: Ki·∫øn th·ª©c t·ªïng qu√°t
- Format: XML
- Download: wget

#### Ti·∫øng Anh

**SQuAD (Stanford Question Answering)**
- URL: https://rajpurkar.github.io/SQuAD-explorer/
- N·ªôi dung: 100K+ c√¢u h·ªèi
- Format: JSON

**RACE (Reading Comprehension)**
- URL: http://www.cs.cmu.edu/~glai1/data/race/
- N·ªôi dung: 28K passages + c√¢u h·ªèi
- Format: TXT, JSON

**ARC (AI2 Reasoning Challenge)**
- URL: https://allenai.org/data/arc
- N·ªôi dung: C√¢u h·ªèi khoa h·ªçc
- Format: JSONL

### 5.2. T·∫°o Dataset Ri√™ng

**Script: generate_dataset.py**

```python
import json
from google.generativeai import GenerativeModel

# Initialize Gemini
model = GenerativeModel('gemini-pro')

topics = [
    'To√°n h·ªçc l·ªõp 10',
    'V·∫≠t l√Ω l·ªõp 11',
    'H√≥a h·ªçc l·ªõp 12',
    # ... th√™m topics
]

dataset = []

for topic in topics:
    prompt = f"T·∫°o 10 c√¢u h·ªèi tr·∫Øc nghi·ªám v·ªÅ {topic}"
    response = model.generate_content(prompt)
    
    # Parse v√† l∆∞u
    questions = parse_questions(response.text)
    dataset.extend(questions)

# Save
with open('custom_dataset.json', 'w', encoding='utf-8') as f:
    json.dump(dataset, f, ensure_ascii=False, indent=2)
```

### 5.3. Augment Data (TƒÉng C∆∞·ªùng D·ªØ Li·ªáu)

```python
# Script: augment_data.py
import random

def paraphrase_question(question):
    """Di·ªÖn ƒë·∫°t l·∫°i c√¢u h·ªèi"""
    prompts = [
        f"Vi·∫øt l·∫°i c√¢u h·ªèi sau: {question}",
        f"Di·ªÖn ƒë·∫°t kh√°c: {question}",
    ]
    # Use Gemini to paraphrase
    return gemini.generate(random.choice(prompts))

def generate_similar(question):
    """T·∫°o c√¢u h·ªèi t∆∞∆°ng t·ª±"""
    prompt = f"T·∫°o c√¢u h·ªèi t∆∞∆°ng t·ª±: {question}"
    return gemini.generate(prompt)

# Augment dataset
original = load_dataset('original.json')
augmented = []

for q in original:
    augmented.append(q)  # Original
    augmented.append(paraphrase_question(q))  # Paraphrase
    augmented.append(generate_similar(q))  # Similar

save_dataset(augmented, 'augmented.json')
```

---

## 6. GOOGLE COLAB TRAINING

### 6.1. Setup Colab

**B∆∞·ªõc 1: T·∫°o Notebook M·ªõi**

1. Truy c·∫≠p: https://colab.research.google.com
2. Click "New Notebook"
3. Ch·ªçn Runtime ‚Üí Change runtime type ‚Üí GPU (T4)

**B∆∞·ªõc 2: Upload Dataset**

```python
# Cell 1: Upload files
from google.colab import files
uploaded = files.upload()

# Cell 2: Unzip if needed
!unzip dataset.zip
```

### 6.2. Train CAT Model

```python
# Cell: Install dependencies
!pip install py-irt pandas numpy

# Cell: Load v√† train
import pandas as pd
from py_irt import irt

# Load data
data = pd.read_csv('responses.csv')

# Train
model = irt(data, model='1pl')
difficulties = model.params['difficulty']

# Save
difficulties.to_json('difficulties.json')

# Download result
from google.colab import files
files.download('difficulties.json')
```

### 6.3. Train Anti-Cheat Model

```python
# Cell: Install
!pip install tensorflow opencv-python

# Cell: Upload images
from google.colab import drive
drive.mount('/content/drive')

# Cell: Train (same code as above)
# ... training code ...

# Cell: Convert to TensorFlow.js
!pip install tensorflowjs
!tensorflowjs_converter \
  --input_format keras \
  model.h5 \
  ./tfjs_model

# Cell: Download
!zip -r model.zip ./tfjs_model
files.download('model.zip')
```

### 6.4. Fine-tune LLM (Advanced)

```python
# Cell: Install
!pip install transformers datasets accelerate

# Cell: Load model
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "vinai/phobert-base"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Cell: Prepare data
from datasets import load_dataset
dataset = load_dataset('json', data_files='train.json')

# Cell: Fine-tune
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=4,
    save_steps=1000,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset['train'],
)

trainer.train()

# Cell: Save
model.save_pretrained('./finetuned_model')
tokenizer.save_pretrained('./finetuned_model')
```

---

## 7. FINE-TUNING LLMs

### 7.1. Khi N√†o C·∫ßn Fine-tune?

‚ùå **KH√îNG C·∫¶N** trong h·∫ßu h·∫øt tr∆∞·ªùng h·ª£p!

Prompt engineering (ƒë√£ l√†m) = 80% hi·ªáu qu·∫£ c·ªßa fine-tuning!

‚úÖ **C·∫¶N** khi:
- Domain r·∫•t ƒë·∫∑c th√π (y h·ªçc, lu·∫≠t, ...)
- C√≥ ‚â•10,000 examples ch·∫•t l∆∞·ª£ng cao
- C√≥ ng√¢n s√°ch ($100-1000)

### 7.2. So S√°nh Ph∆∞∆°ng Ph√°p

| Ph∆∞∆°ng Ph√°p | Chi Ph√≠ | Th·ªùi Gian | ƒê·ªô Kh√≥ | K·∫øt Qu·∫£ |
|-------------|---------|-----------|--------|---------|
| **Prompt Engineering** | $0 | 1 gi·ªù | D·ªÖ | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Few-shot Learning** | $0 | 2 gi·ªù | Trung b√¨nh | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Fine-tuning OpenAI** | $20-100 | 1-2 ng√†y | Trung b√¨nh | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Fine-tuning Open Source** | $0 (GPU) | 3-7 ng√†y | Kh√≥ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

### 7.3. Fine-tuning v·ªõi OpenAI

**B∆∞·ªõc 1: Chu·∫©n B·ªã Data**

```json
// Format: JSONL
{"messages": [
  {"role": "system", "content": "B·∫°n l√† gi√°o vi√™n to√°n"},
  {"role": "user", "content": "T·∫°o c√¢u h·ªèi v·ªÅ ƒë·∫°o h√†m"},
  {"role": "assistant", "content": "C√¢u 1: T√≠nh ƒë·∫°o h√†m..."}
]}
{"messages": [...]}
```

**B∆∞·ªõc 2: Upload v√† Train**

```bash
# Upload file
openai api files.create \
  -f training_data.jsonl \
  -p fine-tune

# Start fine-tune
openai api fine_tunes.create \
  -t file-xxx \
  -m gpt-3.5-turbo \
  --suffix "math-teacher"

# Monitor
openai api fine_tunes.follow -i ft-xxx

# Cost: ~$0.008/1K tokens = $8 cho 1M tokens
```

**B∆∞·ªõc 3: S·ª≠ D·ª•ng**

```javascript
// Update apiEndpoint trong aiQuestionGenerator.ts
const model = 'ft:gpt-3.5-turbo:your-fine-tuned-model';
```

### 7.4. Fine-tuning Open Source (Colab)

```python
# Colab: Fine-tune Gemma 2B
!pip install transformers peft accelerate bitsandbytes

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model

# Load model
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2b",
    load_in_4bit=True  # QLoRA - ti·∫øt ki·ªám RAM
)

# Configure LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
)

model = get_peft_model(model, lora_config)

# Train (same as above)
# ...

# Merge LoRA weights
model = model.merge_and_unload()
model.save_pretrained('./finetuned_gemma')
```

---

## T√ìM T·∫ÆT & KHUY·∫æN NGH·ªä

### ‚úÖ KHUY·∫æN NGH·ªä CHO M·ªåI NG∆Ø·ªúI

1. **B·∫Øt ƒë·∫ßu**: D√πng Gemini (mi·ªÖn ph√≠) ‚Üí [GEMINI_SETUP.md](./GEMINI_SETUP.md)
2. **CAT**: Manual calibration ‚Üí ƒê·ªß t·ªët!
3. **Anti-Cheat**: D√πng BlazeFace ‚Üí Ho√†n h·∫£o!
4. **Essay Grading**: Gemini + prompt engineering ‚Üí Xu·∫•t s·∫Øc!

### ‚ö†Ô∏è CH·ªà TRAIN KHI C·∫¶N

- CAT: Sau 3-6 th√°ng, c√≥ 100+ h·ªçc sinh
- Anti-Cheat: M√¥i tr∆∞·ªùng ƒë·∫∑c bi·ªát
- LLM: Domain c·ª±c k·ª≥ ƒë·∫∑c th√π + c√≥ ng√¢n s√°ch

### üìö T√ÄI NGUY√äN H·ªåC T·∫¨P

**Video Tutorials**:
- TensorFlow.js: https://youtube.com/@TensorFlow
- Colab Training: https://youtube.com/colab-training
- Fine-tuning: https://youtube.com/huggingface

**Courses**:
- Coursera: Machine Learning (Andrew Ng)
- Fast.ai: Practical Deep Learning
- Hugging Face: NLP Course

**Communities**:
- r/MachineLearning
- Hugging Face Forums
- TensorFlow Community

---

**C√¢u h·ªèi? M·ªü issue tr√™n GitHub!**

Next: [GEMINI_SETUP.md](./GEMINI_SETUP.md) - H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng Gemini mi·ªÖn ph√≠
