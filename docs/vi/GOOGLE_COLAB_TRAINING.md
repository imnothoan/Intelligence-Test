# H∆∞·ªõng D·∫´n Training AI tr√™n Google Colab (MI·ªÑN PH√ç) ‚òÅÔ∏è

## M·ª•c L·ª•c
1. [Gi·ªõi Thi·ªáu Google Colab](#1-gi·ªõi-thi·ªáu)
2. [Setup v√† B·∫Øt ƒê·∫ßu](#2-setup)
3. [Training CAT Model](#3-training-cat)
4. [Training Anti-Cheat Model](#4-training-anti-cheat)
5. [Tips & Tricks](#5-tips-tricks)
6. [Troubleshooting](#6-troubleshooting)

---

## 1. Gi·ªõi Thi·ªáu Google Colab

### 1.1. Google Colab L√† G√¨?

**Google Colaboratory (Colab)** l√† m√¥i tr∆∞·ªùng Jupyter Notebook mi·ªÖn ph√≠ ch·∫°y tr√™n cloud c·ªßa Google.

**∆Øu ƒëi·ªÉm:**
- ‚úÖ **Ho√†n to√†n MI·ªÑN PH√ç** 
- ‚úÖ **GPU mi·ªÖn ph√≠** (Tesla T4, P100 ho·∫∑c K80)
- ‚úÖ **RAM 12-16GB** (nhi·ªÅu h∆°n MacBook)
- ‚úÖ **Pre-installed libraries** (TensorFlow, PyTorch, v.v.)
- ‚úÖ **Kh√¥ng c·∫ßn c√†i ƒë·∫∑t g√¨** tr√™n m√°y
- ‚úÖ **Truy c·∫≠p m·ªçi l√∫c** t·ª´ browser

**Gi·ªõi h·∫°n phi√™n b·∫£n FREE:**
- ‚ö†Ô∏è **12 gi·ªù/session** (sau ƒë√≥ b·ªã disconnect)
- ‚ö†Ô∏è **Idle timeout**: 90 ph√∫t kh√¥ng ho·∫°t ƒë·ªông
- ‚ö†Ô∏è **GPU kh√¥ng ƒë·∫£m b·∫£o** (c√≥ th·ªÉ h·∫øt quota)
- ‚ö†Ô∏è **Storage t·∫°m th·ªùi** (files x√≥a sau khi disconnect)

**Colab Pro ($9.99/th√°ng) - N√¢ng cao:**
- 24 gi·ªù/session
- GPU ∆∞u ti√™n (faster GPUs)
- More RAM (up to 32GB)
- Background execution

**K·∫øt lu·∫≠n: FREE version ƒë·ªß d√πng cho Intelligence Test!**

### 1.2. Khi N√†o D√πng Colab?

‚úÖ **D√πng Colab khi:**
- Training anti-cheat models (computer vision)
- CAT calibration v·ªõi dataset l·ªõn (>10,000 records)
- Experiment nhi·ªÅu hyperparameters
- Machine learning tasks n·∫∑ng

‚ùå **Kh√¥ng d√πng Colab khi:**
- Development/debugging code (d√πng MacBook)
- Quick experiments nh·ªè
- Tasks kh√¥ng c·∫ßn GPU

---

## 2. Setup v√† B·∫Øt ƒê·∫ßu

### 2.1. Truy C·∫≠p Google Colab

1. **M·ªü tr√¨nh duy·ªát**
   - Chrome, Safari, Firefox (khuy·∫øn ngh·ªã Chrome)

2. **Truy c·∫≠p Colab**
   - URL: https://colab.research.google.com

3. **ƒêƒÉng nh·∫≠p Google**
   - D√πng t√†i kho·∫£n Gmail c·ªßa b·∫°n
   - Cho ph√©p quy·ªÅn truy c·∫≠p

4. **T·∫°o Notebook m·ªõi**
   - Click "New notebook" ho·∫∑c "File ‚Üí New notebook"
   - Notebook s·∫Ω t·ª± ƒë·ªông save v√†o Google Drive

### 2.2. Enable GPU (Quan Tr·ªçng!)

```
1. Click "Runtime" (Th·ªùi gian ch·∫°y) tr√™n menu bar
2. Click "Change runtime type" (Thay ƒë·ªïi lo·∫°i th·ªùi gian ch·∫°y)
3. Hardware accelerator: Ch·ªçn "GPU"
4. GPU type: "T4" (n·∫øu c√≥ option)
5. Click "Save"
```

**Verify GPU:**
```python
# Cell 1: Ki·ªÉm tra GPU
import tensorflow as tf

print("GPU Available:", tf.config.list_physical_devices('GPU'))
print("TensorFlow version:", tf.__version__)

# N·∫øu c√≥ GPU, s·∫Ω th·∫•y:
# GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
```

### 2.3. Mount Google Drive (L∆∞u Files)

```python
# Cell 2: Mount Google Drive ƒë·ªÉ l∆∞u k·∫øt qu·∫£
from google.colab import drive
drive.mount('/content/drive')

# S·∫Ω hi·ªán link ƒë·ªÉ authorize
# Click link ‚Üí Ch·ªçn t√†i kho·∫£n ‚Üí Copy code ‚Üí Paste v√†o Colab
# Output: Mounted at /content/drive
```

**File structure sau khi mount:**
```
/content/drive/MyDrive/
‚îú‚îÄ‚îÄ Intelligence-Test/
‚îÇ   ‚îú‚îÄ‚îÄ datasets/
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ results/
```

### 2.4. Install Dependencies

```python
# Cell 3: Install th∆∞ vi·ªán (n·∫øu c·∫ßn)
!pip install opencv-python-headless
!pip install pillow

# Verify
import cv2
print("OpenCV version:", cv2.__version__)
```

---

## 3. Training CAT Model Tr√™n Colab

### 3.1. Upload Dataset

**Option 1: Upload tr·ª±c ti·∫øp**
```python
# Cell 4: Upload file
from google.colab import files

print("üì§ Please select your CSV file...")
uploaded = files.upload()

# L·∫•y t√™n file
filename = list(uploaded.keys())[0]
print(f"‚úÖ Uploaded: {filename}")

# Load data
import pandas as pd
df = pd.read_csv(filename)
print(f"üìä Loaded {len(df)} records")
print(df.head())
```

**Option 2: T·ª´ Google Drive**
```python
# Cell 4: Load t·ª´ Google Drive
import pandas as pd

# ƒê∆∞·ªùng d·∫´n file trong Drive
csv_path = '/content/drive/MyDrive/Intelligence-Test/datasets/student_responses.csv'

df = pd.read_csv(csv_path)
print(f"üìä Loaded {len(df)} records")
print(df.head())
```

**Option 3: Download t·ª´ URL**
```python
# Cell 4: Download t·ª´ internet
import pandas as pd

url = "https://your-server.com/data/student_responses.csv"
df = pd.read_csv(url)
print(f"üìä Loaded {len(df)} records")
```

### 3.2. CAT Calibration Script

```python
# Cell 5: CAT Calibration
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import json
from datetime import datetime

def calculate_question_difficulty(df):
    """Calculate difficulty for each question"""
    print("üîç Calculating question difficulties...")
    
    difficulty_map = {}
    
    for q_id in df['question_id'].unique():
        q_responses = df[df['question_id'] == q_id]
        correct_rate = q_responses['is_correct'].mean()
        
        # Difficulty = 1 - correct_rate
        difficulty = 1 - correct_rate
        difficulty = max(0.1, min(0.9, difficulty))
        
        difficulty_map[str(q_id)] = {
            'difficulty': round(difficulty, 3),
            'total_responses': len(q_responses),
            'correct_rate': round(correct_rate, 3)
        }
    
    print(f"‚úÖ Calculated difficulties for {len(difficulty_map)} questions")
    return difficulty_map

def estimate_student_abilities(df):
    """Estimate student ability using IRT"""
    print("üéì Estimating student abilities...")
    
    abilities = {}
    
    for student_id in df['student_id'].unique():
        student_responses = df[df['student_id'] == student_id]
        correct_responses = student_responses[student_responses['is_correct'] == 1]
        
        if len(correct_responses) > 0:
            ability = correct_responses['difficulty'].mean()
        else:
            ability = 0.3
        
        abilities[str(student_id)] = round(ability, 3)
    
    print(f"‚úÖ Estimated abilities for {len(abilities)} students")
    return abilities

def plot_results(difficulty_map, abilities):
    """Create visualizations"""
    print("üìä Creating visualizations...")
    
    fig, axes = plt.subplots(1, 2, figsize=(15, 5))
    
    # Plot 1: Difficulty distribution
    difficulties = [d['difficulty'] for d in difficulty_map.values()]
    axes[0].hist(difficulties, bins=20, edgecolor='black', alpha=0.7, color='skyblue')
    axes[0].set_xlabel('Difficulty')
    axes[0].set_ylabel('Number of Questions')
    axes[0].set_title('Question Difficulty Distribution')
    axes[0].axvline(np.mean(difficulties), color='red', linestyle='--', 
                    label=f'Mean: {np.mean(difficulties):.3f}')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # Plot 2: Ability distribution
    ability_values = list(abilities.values())
    axes[1].hist(ability_values, bins=20, edgecolor='black', alpha=0.7, color='lightgreen')
    axes[1].set_xlabel('Ability')
    axes[1].set_ylabel('Number of Students')
    axes[1].set_title('Student Ability Distribution')
    axes[1].axvline(np.mean(ability_values), color='red', linestyle='--',
                    label=f'Mean: {np.mean(ability_values):.3f}')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('cat_calibration_results.png', dpi=300, bbox_inches='tight')
    print("‚úÖ Saved visualization: cat_calibration_results.png")
    plt.show()

def save_results(difficulty_map, abilities):
    """Save calibration results"""
    print("üíæ Saving results...")
    
    results = {
        'calibration_date': datetime.now().isoformat(),
        'total_questions': len(difficulty_map),
        'total_students': len(abilities),
        'questions': difficulty_map,
        'students': abilities,
        'statistics': {
            'mean_difficulty': round(np.mean([d['difficulty'] for d in difficulty_map.values()]), 3),
            'std_difficulty': round(np.std([d['difficulty'] for d in difficulty_map.values()]), 3),
            'min_difficulty': round(min([d['difficulty'] for d in difficulty_map.values()]), 3),
            'max_difficulty': round(max([d['difficulty'] for d in difficulty_map.values()]), 3),
            'mean_ability': round(np.mean(list(abilities.values())), 3),
            'std_ability': round(np.std(list(abilities.values())), 3),
        }
    }
    
    # Save to JSON
    with open('cat_calibration.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    # Save to Google Drive
    drive_path = '/content/drive/MyDrive/Intelligence-Test/results/cat_calibration.json'
    with open(drive_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print("‚úÖ Results saved to:")
    print("   - cat_calibration.json (local)")
    print(f"   - {drive_path} (Google Drive)")
    
    return results

# Main execution
print("=" * 70)
print("  CAT MODEL CALIBRATION - Google Colab")
print("=" * 70)
print()

# Calibrate
difficulty_map = calculate_question_difficulty(df)
abilities = estimate_student_abilities(df)

# Visualize
plot_results(difficulty_map, abilities)

# Save
results = save_results(difficulty_map, abilities)

# Summary
print()
print("=" * 70)
print("  CALIBRATION SUMMARY")
print("=" * 70)
print(f"üìä Questions calibrated: {results['total_questions']}")
print(f"üë• Students analyzed: {results['total_students']}")
print(f"üìà Difficulty: {results['statistics']['mean_difficulty']:.3f} ¬± {results['statistics']['std_difficulty']:.3f}")
print(f"üìà Range: [{results['statistics']['min_difficulty']:.3f}, {results['statistics']['max_difficulty']:.3f}]")
print(f"üéì Ability: {results['statistics']['mean_ability']:.3f} ¬± {results['statistics']['std_ability']:.3f}")
print()
print("‚úÖ Calibration complete!")
print()
```

### 3.3. Download Results

```python
# Cell 6: Download results to your computer
from google.colab import files

files.download('cat_calibration.json')
files.download('cat_calibration_results.png')

print("‚úÖ Files downloaded to your Downloads folder")
print("üìÅ Import cat_calibration.json into your app!")
```

---

## 4. Training Anti-Cheat Model

### 4.1. Upload Training Data

```python
# Cell 7: Upload dataset
from google.colab import files
import zipfile
import os

print("üì§ Upload your anti-cheat dataset (ZIP file)...")
print("Expected structure:")
print("  dataset.zip")
print("    ‚îú‚îÄ‚îÄ normal/       (500+ images)")
print("    ‚îú‚îÄ‚îÄ looking_away/ (200+ images)")
print("    ‚îú‚îÄ‚îÄ multiple/     (100+ images)")
print("    ‚îî‚îÄ‚îÄ no_face/      (100+ images)")
print()

uploaded = files.upload()

# Extract ZIP
zip_filename = list(uploaded.keys())[0]
with zipfile.ZipFile(zip_filename, 'r') as zip_ref:
    zip_ref.extractall('dataset')

print("‚úÖ Dataset extracted to 'dataset/' folder")

# Count images
for folder in ['normal', 'looking_away', 'multiple', 'no_face']:
    path = f'dataset/{folder}'
    if os.path.exists(path):
        count = len(os.listdir(path))
        print(f"  {folder}: {count} images")
```

### 4.2. Data Preprocessing

```python
# Cell 8: Prepare data
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
import matplotlib.pyplot as plt

# Image parameters
IMG_SIZE = 224
BATCH_SIZE = 32

# Data augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest',
    validation_split=0.2  # 80% train, 20% validation
)

# Training set
train_generator = train_datagen.flow_from_directory(
    'dataset',
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training'
)

# Validation set
val_generator = train_datagen.flow_from_directory(
    'dataset',
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation'
)

print("‚úÖ Data prepared")
print(f"Training samples: {train_generator.samples}")
print(f"Validation samples: {val_generator.samples}")
print(f"Classes: {train_generator.class_indices}")
```

### 4.3. Build Model

```python
# Cell 9: Create CNN model
from tensorflow.keras import layers, models
from tensorflow.keras.applications import MobileNetV2

def create_anti_cheat_model(num_classes=4):
    """
    Create anti-cheat detection model using transfer learning
    Classes: normal, looking_away, multiple, no_face
    """
    
    # Use pre-trained MobileNetV2 (faster on GPU)
    base_model = MobileNetV2(
        input_shape=(IMG_SIZE, IMG_SIZE, 3),
        include_top=False,
        weights='imagenet'
    )
    
    # Freeze base model
    base_model.trainable = False
    
    # Add custom layers
    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation='softmax')
    ])
    
    return model

# Create model
model = create_anti_cheat_model(num_classes=4)

# Compile
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

print("‚úÖ Model created successfully")
```

### 4.4. Train Model

```python
# Cell 10: Train model
import time

print("üöÄ Starting training...")
print("=" * 70)

# Callbacks
callbacks = [
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=5,
        restore_best_weights=True
    ),
    keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=3,
        min_lr=1e-7
    ),
    keras.callbacks.ModelCheckpoint(
        'best_model.h5',
        monitor='val_accuracy',
        save_best_only=True
    )
]

# Train
start_time = time.time()

history = model.fit(
    train_generator,
    epochs=20,
    validation_data=val_generator,
    callbacks=callbacks,
    verbose=1
)

training_time = time.time() - start_time

print()
print("=" * 70)
print(f"‚úÖ Training complete in {training_time/60:.2f} minutes")
print(f"Best validation accuracy: {max(history.history['val_accuracy']):.4f}")
```

### 4.5. Evaluate and Visualize

```python
# Cell 11: Plot training history
plt.figure(figsize=(12, 4))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train')
plt.plot(history.history['val_accuracy'], label='Validation')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train')
plt.plot(history.history['val_loss'], label='Validation')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.savefig('training_history.png', dpi=300)
plt.show()

print("‚úÖ Training visualization saved")
```

### 4.6. Convert to TensorFlow.js

```python
# Cell 12: Convert model for web deployment
!pip install tensorflowjs

import tensorflowjs as tfjs

# Convert
tfjs.converters.save_keras_model(model, 'tfjs_model')

print("‚úÖ Model converted to TensorFlow.js format")
print("üìÅ Model saved in 'tfjs_model/' folder")

# Zip for download
!zip -r tfjs_model.zip tfjs_model/

# Download
from google.colab import files
files.download('tfjs_model.zip')

print("‚úÖ Download complete!")
print("üì¶ Extract tfjs_model.zip to your project's /public/models/ folder")
```

### 4.7. Save to Google Drive

```python
# Cell 13: Backup to Google Drive
import shutil

drive_models_path = '/content/drive/MyDrive/Intelligence-Test/models/'

# Create directory if not exists
!mkdir -p "{drive_models_path}"

# Copy files
shutil.copy('best_model.h5', f'{drive_models_path}anti_cheat_model.h5')
shutil.copytree('tfjs_model', f'{drive_models_path}tfjs_model', dirs_exist_ok=True)
shutil.copy('training_history.png', f'{drive_models_path}training_history.png')

print("‚úÖ Models backed up to Google Drive")
print(f"üìÅ Location: {drive_models_path}")
```

---

## 5. Tips & Tricks

### 5.1. Prevent Timeout

**Problem:** Colab disconnect sau 90 ph√∫t idle

**Solutions:**

**Option 1: Auto-click (JavaScript)**
```javascript
// M·ªü Console (F12) v√† paste code n√†y
function KeepAlive() {
  console.log("Keeping session alive...");
  document.querySelector("colab-connect-button").click();
}
setInterval(KeepAlive, 60000); // Every 1 minute
```

**Option 2: Print progress**
```python
# Trong training loop
import time

for epoch in range(EPOCHS):
    print(f"Epoch {epoch+1}/{EPOCHS} - {time.strftime('%H:%M:%S')}")
    # Training code...
```

**Option 3: Use callbacks**
```python
class KeepAliveCallback(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        print(f"‚úì Epoch {epoch+1} complete at {time.strftime('%H:%M:%S')}")

# Add to callbacks list
callbacks.append(KeepAliveCallback())
```

### 5.2. Monitor GPU Usage

```python
# Cell: Check GPU memory
!nvidia-smi

# Output shows:
# - GPU type (T4, K80, P100)
# - Memory used/total
# - GPU utilization %
```

### 5.3. Speed Up Training

**1. Use smaller image size:**
```python
IMG_SIZE = 128  # Instead of 224 ‚Üí 2-3x faster
```

**2. Increase batch size:**
```python
BATCH_SIZE = 64  # Instead of 32 ‚Üí 1.5x faster
# But watch GPU memory!
```

**3. Use MobilNet instead of ResNet:**
```python
# MobileNetV2: Fast, good enough
# ResNet50: Slower, slightly better accuracy
```

**4. Mixed precision:**
```python
from tensorflow.keras import mixed_precision
mixed_precision.set_global_policy('mixed_float16')
# ‚Üí 2-3x faster on modern GPUs
```

### 5.4. Save Checkpoints Frequently

```python
# Save every 5 epochs
checkpoint_callback = keras.callbacks.ModelCheckpoint(
    'model_epoch_{epoch:02d}.h5',
    save_freq='epoch',
    period=5
)
```

---

## 6. Troubleshooting

### 6.1. GPU Not Available

**Problem:** `GPU Available: []`

**Solutions:**
```python
# 1. Check runtime type
# Runtime ‚Üí Change runtime type ‚Üí GPU

# 2. Check quota
# B·∫°n c√≥ th·ªÉ h·∫øt GPU quota (d√πng qu√° nhi·ªÅu)
# ƒê·ª£i v√†i gi·ªù ho·∫∑c d√πng t√†i kho·∫£n kh√°c

# 3. Restart runtime
# Runtime ‚Üí Restart runtime

# 4. Use CPU as fallback
# Training s·∫Ω ch·∫≠m h∆°n nh∆∞ng v·∫´n ho·∫°t ƒë·ªông
```

### 6.2. Out of Memory

**Problem:** `ResourceExhaustedError: OOM when allocating tensor`

**Solutions:**
```python
# 1. Reduce batch size
BATCH_SIZE = 16  # Or even 8

# 2. Reduce image size
IMG_SIZE = 128

# 3. Clear memory
import gc
import keras.backend as K

K.clear_session()
gc.collect()

# 4. Use mixed precision
from tensorflow.keras import mixed_precision
mixed_precision.set_global_policy('mixed_float16')
```

### 6.3. Session Disconnected

**Problem:** "Runtime disconnected"

**Solutions:**
```python
# 1. Files ƒë√£ save trong Google Drive v·∫´n c√≤n
# Mount l·∫°i Drive v√† load checkpoint:

from google.colab import drive
drive.mount('/content/drive')

# Load checkpoint
model = keras.models.load_model('/content/drive/MyDrive/.../best_model.h5')

# Resume training t·ª´ checkpoint
history = model.fit(train_gen, epochs=REMAINING_EPOCHS, ...)
```

### 6.4. Import Error

**Problem:** `ModuleNotFoundError: No module named 'xyz'`

**Solution:**
```python
!pip install xyz

# Example:
!pip install opencv-python-headless
!pip install pillow
!pip install tensorflowjs
```

---

## 7. Complete Workflow Example

### 7.1. Quick Start Notebook

```python
# ===== CELL 1: Setup =====
print("üöÄ Intelligence Test - Anti-Cheat Model Training")
print("=" * 70)

# Mount Drive
from google.colab import drive
drive.mount('/content/drive')

# Check GPU
import tensorflow as tf
print(f"GPU Available: {tf.config.list_physical_devices('GPU')}")
print(f"TensorFlow: {tf.__version__}")

# ===== CELL 2: Upload Data =====
from google.colab import files
import zipfile

uploaded = files.upload()
zip_file = list(uploaded.keys())[0]

with zipfile.ZipFile(zip_file, 'r') as zip_ref:
    zip_ref.extractall('dataset')

print("‚úÖ Dataset ready")

# ===== CELL 3: Prepare Data =====
from tensorflow.keras.preprocessing.image import ImageDataGenerator

IMG_SIZE = 224
BATCH_SIZE = 32

datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    # ... augmentation params
)

train_gen = datagen.flow_from_directory(
    'dataset',
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training'
)

val_gen = datagen.flow_from_directory(
    'dataset',
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation'
)

# ===== CELL 4: Build Model =====
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras import layers, models

base = MobileNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3),
                   include_top=False, weights='imagenet')
base.trainable = False

model = models.Sequential([
    base,
    layers.GlobalAveragePooling2D(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(4, activation='softmax')
])

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# ===== CELL 5: Train =====
history = model.fit(
    train_gen,
    epochs=20,
    validation_data=val_gen,
    callbacks=[...]
)

# ===== CELL 6: Convert & Download =====
import tensorflowjs as tfjs
tfjs.converters.save_keras_model(model, 'tfjs_model')

!zip -r tfjs_model.zip tfjs_model/
files.download('tfjs_model.zip')

print("‚úÖ Complete!")
```

---

## 8. K·∫øt Lu·∫≠n

### Summary

**Google Colab l√† l·ª±a ch·ªçn T·ªêT NH·∫§T cho:**
- ‚úÖ Training anti-cheat models (computer vision)
- ‚úÖ CAT calibration v·ªõi dataset l·ªõn
- ‚úÖ Free GPU ‚Üí Nhanh h∆°n 5-10x so v·ªõi MacBook
- ‚úÖ Kh√¥ng c·∫ßn c√†i ƒë·∫∑t, truy c·∫≠p m·ªçi l√∫c

**Workflow:**
```
1. Chu·∫©n b·ªã data tr√™n m√°y local
2. Upload l√™n Colab (ho·∫∑c l∆∞u trong Drive)
3. Enable GPU
4. Train model (20-30 ph√∫t)
5. Download results
6. Deploy v√†o app
```

**Next Steps:**
- [ ] Th·ª±c h√†nh v·ªõi CAT calibration
- [ ] Th·ª≠ training anti-cheat model
- [ ] ƒê·ªçc th√™m: [TensorFlow.js Integration](./TENSORFLOW_JS.md)

**Happy Training on Colab! ‚òÅÔ∏èüöÄ**
